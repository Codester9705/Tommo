{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codester9705/Tommo/blob/main/Personal_Copy_Team_I26_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kOHuw4dEVvW"
      },
      "source": [
        "**Package Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqYHzE3eI9u4"
      },
      "outputs": [],
      "source": [
        "# Ngrok (Python Flask) Method:\n",
        "# !pip install flask-ngrok\n",
        "# !ngrok authtoken 2yM8OQsUHtfI09y48Lmoki1F9Zc_3m9iXgfbv4n1HdpRtaS6Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjWbjdKRMsGj"
      },
      "outputs": [],
      "source": [
        "# Install Streamlit related packages\n",
        "!pip install streamlit\n",
        "!pip install streamlit streamlit_option_menu\n",
        "!pip install -q streamlit\n",
        "\n",
        "# Install localtunnel for exposing the app\n",
        "!npm install localtunnel\n",
        "\n",
        "# Create the 'pages' directory where your multi-page app files will reside\n",
        "!mkdir -p pages\n",
        "\n",
        "# For Facial Recognition\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP52oN04I_kq"
      },
      "source": [
        "**Home Page**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rWMNWo0KtEK"
      },
      "outputs": [],
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "# Set Streamlit page configuration\n",
        "# page_title sets the browser tab title AND the label for this page in the sidebar\n",
        "st.set_page_config(\n",
        "    page_title=\"Home\", # This will be the label for the Home.py page in the sidebar\n",
        "    layout=\"centered\", # Sets the page layout to centered\n",
        "    initial_sidebar_state=\"expanded\", # Keeps the sidebar expanded by default\n",
        ")\n",
        "\n",
        "# --- Content for the Home Page ---\n",
        "st.title(\"Welcome to the Home Page! üè†\")\n",
        "st.write(\"This is the main entry point of our Streamlit application.\")\n",
        "st.markdown(\"\"\"\n",
        "    Hello there! You've landed on the **Home Page**.\n",
        "    Use the sidebar on the left to navigate to other sections.\n",
        "\"\"\")\n",
        "\n",
        "st.info(\"To see the navigation, click the `>` icon in the top-left corner of the page (if the sidebar is hidden).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Page**"
      ],
      "metadata": {
        "id": "sQ21OeZ60yyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/1_About.py\n",
        "import streamlit as st\n",
        "\n",
        "# Set page config for consistency across pages (optional, but good practice)\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"About Tommo: Your Digital Companion üí°\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    At [Your Hackathon Team Name - e.g., Team A.I. for Good], we envision a future where emotional well-being is accessible and proactively supported for everyone.\n",
        "    Introducing **Tommo**, your innovative digital companion designed to elevate mental health support, especially for the disadvantaged and disabled in our society.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"What is Tommo?\")\n",
        "st.markdown(\"\"\"\n",
        "    Tommo, named after the Japanese word for friend ('Tomodachi'), embodies true companionship.\n",
        "    It's more than just an application; it's a reliable touchpoint that helps individuals document their journey towards better mental well-being.\n",
        "    Our solution acts not just as a bridge between an individual and happiness, but also as a secure space to record feelings and expressions, serving as an impetus to better mental health.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"How Tommo Empowers & Assists\")\n",
        "st.markdown(\"\"\"\n",
        "    Tommo leverages cutting-edge AI, including sophisticated **facial and speech recognition** capabilities, to understand and interact with users empathetically.\n",
        "    At its core, a powerful **LLM (Large Language Model) powered by PyTorch** ensures secure, private, and helpful conversations, fostering a genuine sense of connection.\n",
        "\n",
        "    ### Reducing Therapists' Initial Efforts\n",
        "    A core aspect of Tommo's innovation is its ability to support mental health professionals. By consistently recording an individual's emotional states, expressions, and conversational insights over time, Tommo provides therapists with invaluable, objective data. This rich context is available *before* the first session, allowing therapists to:\n",
        "    * **Understand the user's emotional baseline and fluctuations.**\n",
        "    * **Identify recurring patterns or triggers.**\n",
        "    * **Tailor initial interventions more effectively.**\n",
        "    This significantly reduces the initial investigative effort for therapists, enabling them to focus on targeted, impactful therapeutic work from the outset, leading to more efficient and personalized care.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"Our Vision: Empowering Through AI\")\n",
        "st.markdown(\"\"\"\n",
        "    Tommo directly addresses the guiding question of the Hackathon:\n",
        "    _\"How may we improve accessibility and empower the disadvantaged and disabled in our society?\"_\n",
        "    By providing an accessible, non-judgmental, and technologically advanced platform, Tommo empowers individuals to proactively engage with their emotional well-being, breaking down barriers to mental health support.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "LnRBKSB40yaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contact Us Page**"
      ],
      "metadata": {
        "id": "MZR9EJhv06oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/2_Contact.py\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"Connect with the Tommo Team ü§ù\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    We are passionate about mental wellness and always open to collaboration, feedback, or just a friendly chat about Tommo and the future of AI in mental health.\n",
        "    Your insights are invaluable as we strive to empower communities through innovation.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"General Inquiries\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    Feel free to reach out with any questions about Tommo or our project:\n",
        "    * **Email:** [info@tommo-project.com](mailto:info@tommo-project.com)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "st.subheader(\"Support & Feedback\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    If you have specific feedback or require support regarding Tommo's functionality:\n",
        "    * **Email:** [support@tommo-project.com](mailto:support@tommo-project.com)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "st.subheader(\"Join Our Community & Follow Our Progress\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    Stay updated on Tommo's development and connect with our team:\n",
        "    * **Twitter/X:** [@TommoAI_Official](https://twitter.com/TommoAI_Official) (Follow us for updates!)\n",
        "    * **GitHub:** [Tommo-Project/Tommo-App](https://github.com/Tommo-Project/Tommo-App) (Explore our code!)\n",
        "    * **LinkedIn:** [Tommo AI Solutions](https://linkedin.com/company/tommo-ai-solutions) (Connect with our team members!)\n",
        "    * **Our Project Website:** [www.tommo-project.com](https://www.tommo-project.com) (Coming Soon!)\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "V5Q_fzF4079b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot Page**"
      ],
      "metadata": {
        "id": "euP6hDAR1bNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/3_Chatbot.py\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"Tommo Chatbot: Your LLM Companion üí¨\")\n",
        "st.write(\"Engage in secure and helpful conversations powered by our PyTorch-based LLM.\")"
      ],
      "metadata": {
        "id": "C8EJ70j-1cdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Facial Recognition**"
      ],
      "metadata": {
        "id": "z5w0aF8H2z2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Create Streamlit App Page File (pages/4_Facial_Recognition.py)\n",
        "# This cell will write the content below into the specified file.\n",
        "# You run this AFTER Cell 2 has successfully loaded/trained and saved the model.\n",
        "\n",
        "%%writefile pages/4_Facial_Recognition.py\n",
        "# --- Start of content for pages/4_Facial_Recognition.py ---\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"Facial Expression Recognition: Understanding Your Expressions üëÅÔ∏è\")\n",
        "st.write(\"Upload an image or use your camera to see predicted facial expressions.\")\n",
        "\n",
        "# --- 1. Define the Model Architecture (MUST be identical to training) ---\n",
        "class EmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.relu4(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# --- 2. Load the Trained Model ---\n",
        "@st.cache_resource\n",
        "def load_emotion_model():\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    # Path to your saved model file in Google Drive. Make sure this matches the training script!\n",
        "    model_path = '/content/drive/MyDrive/emotion_cnn_model/emotion_cnn_model_v3.pth' # Using the _v2 path\n",
        "\n",
        "    # These class names must be in the same order as your training dataset\n",
        "    class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'] # Hardcoding for clarity in the written file\n",
        "\n",
        "    model = EmotionCNN(num_classes=len(class_names)).to(device)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval()\n",
        "        st.success(\"Emotion recognition model loaded successfully!\")\n",
        "        return model, class_names\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Error: Model file not found at '{{model_path}}'.\")\n",
        "        st.info(\"Please ensure your trained 'emotion_cnn_model_v3.pth' is in the correct Google Drive path.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {{e}}\")\n",
        "        st.info(\"Please check the model file and its compatibility with the `EmotionCNN` architecture defined.\")\n",
        "        return None, None\n",
        "\n",
        "model, class_names = load_emotion_model()\n",
        "\n",
        "# --- 3. Image Preprocessing for Inference ---\n",
        "transform_inference = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# --- 4. Prediction Function ---\n",
        "def predict_emotion(image_pil, model, class_names):\n",
        "    if model is None:\n",
        "        return \"Model not loaded.\", None\n",
        "\n",
        "    image_tensor = transform_inference(image_pil).unsqueeze(0)\n",
        "    image_tensor = image_tensor.to(torch.device(\"cpu\"))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
        "        predicted_class_idx = torch.argmax(probabilities).item()\n",
        "\n",
        "    predicted_emotion = class_names[predicted_class_idx]\n",
        "    confidence = probabilities[predicted_class_idx].item() * 100\n",
        "\n",
        "    return predicted_emotion, confidence\n",
        "\n",
        "# --- 5. Streamlit UI for Image Input ---\n",
        "if model:\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Analyze an Image\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    camera_image = None\n",
        "    if uploaded_file is None:\n",
        "        camera_image = st.camera_input(\"Or take a picture with your camera...\")\n",
        "\n",
        "    image_to_process = None\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        image_to_process = Image.open(uploaded_file).convert(\"RGB\")\n",
        "        # FIX 1: Change use_column_width to use_container_width\n",
        "        st.image(image_to_process, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "    elif camera_image is not None:\n",
        "        image_to_process = Image.open(io.BytesIO(camera_image.read())).convert(\"RGB\")\n",
        "        # FIX 2: Change use_column_width to use_container_width\n",
        "        st.image(image_to_process, caption=\"Captured Image\", use_container_width=True)\n",
        "\n",
        "    if image_to_process is not None:\n",
        "        st.spinner(\"Analyzing facial expression...\")\n",
        "        predicted_emotion, confidence = predict_emotion(image_to_process, model, class_names)\n",
        "        # FIX 3 & 4: Correct f-string formatting (remove extra curly braces)\n",
        "        st.markdown(f\"### Predicted Emotion: **{predicted_emotion}**\")\n",
        "        st.info(f\"Confidence: {confidence:.2f}%)\") # Corrected f-string here\n",
        "    else:\n",
        "        st.info(\"Upload an image or take a photo to get started!\")\n",
        "# --- End of content for pages/4_Facial_Recognition.py ---"
      ],
      "metadata": {
        "id": "8fSCGkq320FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the Streamlit Application**"
      ],
      "metadata": {
        "id": "5MkeDM5P3kCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd5GGoA6Pw4l"
      },
      "outputs": [],
      "source": [
        "# Run the Streamlit app in the background and expose it via localtunnel\n",
        "# The output will include a URL you can open in your browser\n",
        "!streamlit run Home.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}